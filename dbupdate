import pymysql  # or psycopg2 for PostgreSQL

def chunked_batch_update(connection, table, updates, chunk_size=100):
    cursor = connection.cursor()
    
    for i in range(0, len(updates), chunk_size):
        chunk = updates[i:i + chunk_size]
        
        set_clauses = []
        ids = []
        
        for update in chunk:
            rowid = update["Rowtoupdate"]
            session_id = update["Session id"]
            column_to_update = update["Columntoupdate"]
            value = update["Value"]
            
            # Prepare the SET clause for each row/column update
            case_statement = f"WHEN id = {rowid} THEN '{value}'"
            set_clauses.append(f"{column_to_update} = CASE {case_statement} ELSE {column_to_update} END")
            ids.append(str(rowid))
        
        # Final SQL script
        set_clause_str = ', '.join(set_clauses)
        ids_str = ', '.join(ids)

        sql_script = f"""
        UPDATE {table}
        SET {set_clause_str}
        WHERE id IN ({ids_str});
        """
        
        # Execute the SQL update
        cursor.execute(sql_script)
    
    # Commit changes and close cursor
    connection.commit()
    cursor.close()

# Example usage
updates_list = [
    {"Rowtoupdate": 1, "Session id": "user1", "Columntouodate": "column1", "Value": "new_value1"},
    {"Rowtoupdate": 2, "Session id": "user2", "Columntouodate": "column1", "Value": "new_value2"},
    # Add more rows as needed
]
connection = pymysql.connect(host='localhost', user='user', password='passwd', db='your_db')  # Adjust connection details
chunked_batch_update(connection, 'your_table', updates_list)
connection.close()